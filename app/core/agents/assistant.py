from app.model.message import Message,MessageRead
from app.model.chat import Chat, ChatStatusEnum
from app.model.message_context import ChatContext, ContextType,ChatContextRead
from app.config.database import get_db, SessionLocal
from typing import List, Optional
from fastapi import Depends
from sqlalchemy import select
from sqlalchemy.orm import Session
from qwen_agent.llm.schema import Message as QwenMessage
import json
import logging

import time
import random
import string

# ... (Previous imports are kept as they are needed for the full context if this file was standalone, 
# but specifically for assistant.py, we need the imports below)
import copy
import json
import time
import uuid
from typing import Dict, Iterator, List, Literal, Optional, Union

from qwen_agent.agents.fncall_agent import FnCallAgent
from qwen_agent.llm import BaseChatModel
from qwen_agent.llm.schema import CONTENT, ROLE, SYSTEM, USER, ContentItem, Message  # DEFAULT_SYSTEM_MESSAGE
from qwen_agent.log import logger
from qwen_agent.tools import BaseTool
from app.core.tools.time import get_current_time, get_three_month_ago, get_last_year, get_current_year

from app.core.rag.knowledge_search import (
    KnowledgeSearchService,
    format_knowledge_to_source_and_content
)

from app.core.text_formatter import format_text_for_markdown
import re

# ä¼˜åŒ–åçš„ç³»ç»Ÿæç¤ºè¯ï¼šå¢åŠ äº†æ€ç»´é“¾å¼•å¯¼å’Œæ›´ä¸¥æ ¼çš„é€»è¾‘çº¦æŸ
DEFAULT_SYSTEM_MESSAGE='''ä½ æ˜¯å¦é—¨å¸‚å…¬ç§¯é‡‘æ”¿åŠ¡æœåŠ¡åŠ©æ‰‹å°é‡‘çµã€‚åœ¨å›ç­”ç”¨æˆ·é—®é¢˜ä¹‹å‰ï¼Œè¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ€è€ƒå’Œå›ç­”æµç¨‹ï¼š

## ğŸ’¡ æ€è€ƒä¸å†³ç­–æµç¨‹ (Thought Process)
1.  **æ„å›¾è¯†åˆ«**ï¼šé¦–å…ˆæ˜ç¡®ç”¨æˆ·æ˜¯æƒ³åŠç†ä¸šåŠ¡ï¼ˆæå–ã€è´·æ¬¾ï¼‰ã€æŸ¥è¯¢ä¿¡æ¯ï¼ˆé¢åº¦ã€è¿›åº¦ï¼‰è¿˜æ˜¯å’¨è¯¢æ”¿ç­–æ¡ä»¶ã€‚
2.  **æ¡ä»¶åŒ¹é…**ï¼šæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æä¾›äº†å…³é”®æ¡ä»¶ï¼ˆæˆ·ç±åœ°ã€è´­æˆ¿åœ°ã€å‚ä¿çŠ¶æ€ã€æ—¶é—´èŠ‚ç‚¹ï¼‰ã€‚
    - è‹¥æ¡ä»¶ç¼ºå¤±ä¸”å½±å“åˆ¤æ–­ï¼ˆå¦‚â€œç¦»èŒæå–â€æœªè¯´æ˜æˆ·ç±ï¼‰ï¼Œå¿…é¡»ä¼˜å…ˆç¤¼è²Œè¿½é—®ã€‚
3.  **çŸ¥è¯†éªŒè¯**ï¼šåœ¨ä¸‹æ–‡æä¾›çš„ã€çŸ¥è¯†åº“ã€‘ä¸­å¯»æ‰¾è¯æ®ã€‚
    - **ä¸¥æ ¼åŒ¹é…**ï¼šç¦æ­¢ä»…å‡­å…³é”®è¯åŒ¹é…ï¼Œå¿…é¡»ç¡®è®¤æ”¿ç­–é€‚ç”¨çš„å‰ææ¡ä»¶ï¼ˆå¦‚æ—¶é—´èŒƒå›´ã€é€‚ç”¨äººç¾¤ï¼‰ä¸ç”¨æˆ·æƒ…å†µä¸€è‡´ã€‚
    - **å†²çªè§£å†³**ï¼šè‹¥å¤šæ¡çŸ¥è¯†å­˜åœ¨å†²çªï¼Œä¼˜å…ˆé‡‡ä¿¡ã€è¡ŒåŠ¨æŒ‡å—ã€‘æˆ–å‘å¸ƒæ—¶é—´è¾ƒæ–°çš„æ”¿ç­–ã€‚
4.  **ç­”æ¡ˆæ„å»º**ï¼š
    - ç›´æ¥å›ç­”æ ¸å¿ƒç»“è®ºï¼ˆèƒ½/ä¸èƒ½/éœ€è¦xxææ–™ï¼‰ã€‚
    - åˆ†ç‚¹é™ˆè¿°ç»†èŠ‚ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€‚
    - é™„ä¸Šæ¥æºå¼•ç”¨ã€‚

## æ ¸å¿ƒè§„åˆ™
1.  **ç²¾å‡†è¯†åˆ«ç”¨æˆ·æ ¸å¿ƒè¯‰æ±‚**
    èšç„¦ç”¨æˆ·é—®é¢˜æŒ‡å‘çš„å…¬ç§¯é‡‘ä¸šåŠ¡ç±»å‹ã€‚ç‰¹åˆ«æ³¨æ„åŒºåˆ†ï¼š
    - å…³é”®è¯†åˆ«ï¼š
        - ç”¨æˆ·æé—®â€œåœ¨Xåœ°è´­æˆ¿ï¼Œå¦‚ä½•æå–å…¬ç§¯é‡‘ï¼Ÿâ€ -> **ç†è§£ä¸ºæå–å¦é—¨å…¬ç§¯é‡‘ç”¨äºXåœ°è´­æˆ¿**ã€‚
        - ç”¨æˆ·æé—®â€œæå–Xåœ°å…¬ç§¯é‡‘â€ -> **ç†è§£ä¸ºæå–å¼‚åœ°å…¬ç§¯é‡‘**ï¼ˆé€šå¸¸éœ€å¼•å¯¼è‡³å½“åœ°ä¸­å¿ƒï¼‰ã€‚
2.  **ä¸¥æ ¼åŒºåˆ†æ”¿ç­–é€‚ç”¨èŒƒå›´**
    æ˜ç¡®åŒºåˆ†ï¼šæœ¬å¸‚/çœå†…/çœå¤–ã€æœ¬åœ°æˆ·ç±/å¤–åœ°æˆ·ç±ã€å•ä½èŒå·¥/çµæ´»å°±ä¸šã€‚
    - ä¾‹ï¼šä»£é™…äº’åŠ©ä¸šåŠ¡ä»…é™ 2025.2.1-2025.12.31 æœŸé—´ã€‚
3.  **ä¸»åŠ¨æ’é™¤å¹²æ‰°é¡¹**
    æ’é™¤åŒ»ä¿ã€ç¤¾ä¿ã€ç¨åŠ¡ç­‰éå…¬ç§¯é‡‘å†…å®¹ï¼Œå³ä½¿æ£€ç´¢åˆ°äº†ç›¸å…³ç‰‡æ®µã€‚
4.  **å¸¸è¯†æ€§ä¸æ— å†…å®¹å¤„ç†**
    - å¸¸è¯†æ€§é—®é¢˜ï¼ˆå¦‚â€œè£…ä¿®â€ã€â€œä¹°è½¦ä½â€ï¼‰ç›´æ¥å›ç­”â€œä¸èƒ½â€å¹¶ç®€è¿°åŸå› ã€‚
    - æ— æ³•ç¡®å®šæˆ–æ— ç›¸å…³æ”¿ç­–ï¼šç›´æ¥å›å¤â€œå½“å‰æœªæ£€ç´¢åˆ°ç›´æ¥ç›¸å…³çš„æ”¿ç­–æ¡æ–‡ï¼Œè¯·å’¨è¯¢0592-12345-1-0å…¬ç§¯é‡‘ä¸“å¸­ã€‚â€

## çŸ¥è¯†åº“å†…å®¹å›ç­”åŸåˆ™
1.  **å…ˆç»“è®ºåç»†èŠ‚**ï¼šå…ˆå›ç­”â€œèƒ½â€æˆ–â€œä¸èƒ½â€ï¼Œå†å±•å¼€ã€‚
2.  **æ¥æºæ ¸éªŒ**ï¼šä¸¥ç¦ç¼–é€ çŸ¥è¯†åº“ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚
3.  **å¤šé—®é¢˜åˆ†ç‚¹**ï¼šç”¨æˆ·æ¶‰åŠå¤šä¸ªé—®é¢˜æ—¶ï¼Œåˆ†ç‚¹ä½œç­”ã€‚
4.  **ä¿ç•™å®˜æ–¹é“¾æ¥**ï¼šå›ç­”ä¸­ä¿ç•™å®˜ç½‘é“¾æ¥ã€‚

## å…³é”®ä¸šåŠ¡æ”¿ç­–é€ŸæŸ¥ï¼ˆé«˜é¢‘æ˜“é”™ç‚¹ï¼‰
- **ç¦»èŒ/å¤±ä¸šæå–**ï¼š
    - **å¦é—¨æˆ·ç±**ï¼š**ä¸èƒ½**åŠç†ç¦»èŒ/å¤±ä¸šæå–ï¼ˆå¿…é¡»é€€ä¼‘æˆ–æ»¡è¶³å…¶ä»–æ¡ä»¶ï¼‰ã€‚
    - **å¤–åœ°æˆ·ç±**ï¼šéœ€è´¦æˆ·å°å­˜æ»¡6ä¸ªæœˆï¼Œä¸”æœªåœ¨å¼‚åœ°ç»§ç»­ç¼´å­˜ã€‚
- **è´­æˆ¿æå–**ï¼š
    - **å†²æŠµæœ¬é‡‘**ï¼š**ä»…é™å¦é—¨æœ¬å¸‚**æˆ¿äº§ã€‚
    - **å¼‚åœ°è´­æˆ¿**ï¼š**ä¸èƒ½å†²æŠµæœ¬é‡‘**ï¼Œä»…èƒ½â€œæŒ‰å¹´æå–æŠ¥é”€è´·æ¬¾æœ¬æ¯â€ã€‚
    - **æå–æ¡ä»¶**ï¼šå¼‚åœ°è´­æˆ¿éœ€æ»¡è¶³â€œæˆ·ç±åœ°â€æˆ–â€œå·¥ä½œåœ°â€åœ¨è´­æˆ¿åœ°ã€‚
- **ç§Ÿæˆ¿æå–**ï¼š
    - çº¿ä¸Šæ— æ³•æŸ¥è¯¢ç»ˆæ­¢æ—¶é—´ã€‚
    - ç§Ÿæˆ¿æå–ä¸è´­æˆ¿/è¿˜è´·ä¸šåŠ¡å¯èƒ½äº’æ–¥ï¼Œéœ€æç¤ºç”¨æˆ·ã€‚
- **ä»£é™…äº’åŠ©**ï¼š
    - ä»…é™çˆ¶æ¯ä¸å­å¥³ä¹‹é—´ï¼Œä¸åŒ…æ‹¬å…„å¼Ÿå§å¦¹æˆ–ç¥–å­™ã€‚
- **åŠç†æ¸ é“**ï¼š
    - çº¿ä¸‹æå–ä¸šåŠ¡é€šå¸¸å»**è´·æ¬¾é“¶è¡Œç½‘ç‚¹**ï¼ˆå²›å†…/å²›å¤–åŒºåˆ†ï¼‰ï¼Œè€Œéç¼´å­˜é“¶è¡Œã€‚

## å¼•ç”¨æ ‡æ³¨è§„åˆ™
1.  å¼•ç”¨æ ¼å¼ï¼š[æ¥æº:[æ–‡æ¡£ID](æ–‡æ¡£ID)]ã€‚
2.  ä»…å¼•ç”¨çœŸæ­£æ”¯æŒä½ å›ç­”çš„æ–‡æ¡£ï¼Œä¸è¦å‡‘æ•°ã€‚

## ç¦æ­¢å›ç­”è§„åˆ™
1.  éå…¬ç§¯é‡‘ç›¸å…³è¯é¢˜ï¼ˆæ”¿æ²»ã€å¨±ä¹ç­‰ï¼‰ä¸äºˆå›ç­”ã€‚
2.  æ˜ç¡®å’¨è¯¢â€œå¦‚ä½•æå–å¼‚åœ°å…¬ç§¯é‡‘â€çš„ï¼Œå¼•å¯¼å’¨è¯¢å½“åœ°ã€‚
'''

KNOWLEDGE_TEMPLATE = """# çŸ¥è¯†åº“
{knowledge}"""

KNOWLEDGEGRAPG_TEMPLATE = '''# çŸ¥è¯†å›¾è°± (å…³è”å…³ç³»å‚è€ƒ)
{knowledgegraph}
'''

KNOWLEDGE_SNIPPET = """## æ¥è‡ª {source} çš„å†…å®¹ï¼š

{content}
"""

BASE_INFO_TEMPLATE = """ # åŸºç¡€ä¸Šä¸‹æ–‡ä¿¡æ¯

## æ—¶é—´å‚ç…§
å½“å‰ç³»ç»Ÿæ—¶é—´: {current_time}
è‡³ä»Šä¸‰ä¸ªæœˆå‰ï¼š{three_month}
å»å¹´: {last_year}
ä»Šå¹´: {current_year}
"""

DATA_INFO_TEMPLATE= """ # è¡¨æ ¼æ•°æ®è¯¦æƒ…
- **ä½¿ç”¨è¯´æ˜**ï¼šä»¥ä¸‹æ•°æ®æ¥è‡ªæ”¿ç­–è¡¨æ ¼ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ä½œç­”ã€‚
{data}
"""


class Assistant(FnCallAgent):
    """This is a widely applicable agent integrated with RAG capabilities and function call ability."""

    def __init__(self,
                 function_list: Optional[List[Union[str, Dict, BaseTool]]] = None,
                 llm: Optional[Union[Dict, BaseChatModel]] = None,
                 system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,
                 name: Optional[str] = None,
                 description: Optional[str] = None,
                 files: Optional[List[str]] = None,
                 rag_cfg: Optional[Dict] = None):
        
        super().__init__(function_list=function_list,
                         llm=llm,
                         system_message=system_message,
                         name=name,
                         description=description,
                         files=files,
                         rag_cfg=rag_cfg)
        self.full_text = ""
        self.current_knowledge = ""
        self.supp_text = ""
        self.knowledge_data = {}
        self.sources = []


    def _run(self,
             messages: List[Message],
             lang: Literal['en', 'zh'] = 'zh',
             knowledge: str = '',
             **kwargs) -> Iterator[List[Message]]:
        """Q&A with RAG and tool use abilities.

        Args:
            knowledge: If an external knowledge string is provided,
              it will be used directly without retrieving information from files in messages.

        """

        new_messages = self._prepend_knowledge_prompt(messages=messages, lang=lang, knowledge=knowledge, **kwargs)
        return super()._run(messages=new_messages, lang=lang, **kwargs)

    def _prepend_knowledge_prompt(self,
                                  messages: List[Message],
                                  knowledge: str = '',
                                  **kwargs) -> List[Message]:
        messages = copy.deepcopy(messages)
        response_keywords = []
        query = None

        if not knowledge:
            query = KnowledgeSearchService.extract_query_from_messages(messages)

        # çŸ¥è¯†åº“æ£€ç´¢
        knowledge_graph_prompt=""
        excel_data_prompt = ""
        if not knowledge and query:
            # ä½¿ç”¨ç»Ÿä¸€çš„çŸ¥è¯†æ£€ç´¢æœåŠ¡
            # [ä¿®æ”¹ç‚¹ 1 & 2] å¢åŠ  doc_top_n æ•°é‡ï¼Œå¯ç”¨çŸ¥è¯†å›¾è°±æœç´¢
            knowledge_data, graph_data, excel_data = KnowledgeSearchService.search_and_integrate_knowledge(
                query=query,
                doc_top_n=10,        # å¢åŠ å¬å›æ•°é‡ï¼ŒåŸä¸º 5
                graph_top_n=3,
                enable_graph_search=True # å¯ç”¨å›¾è°±æœç´¢ï¼Œå¢å¼ºå¤æ‚é—®é¢˜æ¨ç†
            )

            if knowledge_data:
                knowledge = KnowledgeSearchService.format_knowledge_for_prompt(knowledge_data)

                self.knowledge_data = knowledge_data

            if graph_data:
                knowledge_graph_prompt = KNOWLEDGEGRAPG_TEMPLATE.format(knowledgegraph=graph_data)
            
            if excel_data:
                excel_data_prompt = DATA_INFO_TEMPLATE.format(data=excel_data)
                
        if knowledge:
            knowledge_prompt = format_knowledge_to_source_and_content(knowledge)
        else:
            knowledge_prompt = []

        
        
        snippets = []
        references = {}
        for k in knowledge_prompt:
            snippets.append(KNOWLEDGE_SNIPPET.format(source=k['source'], content=k['content']))
            references[k['source']] = k['content']
        knowledge_prompt = ''
        if snippets:
            knowledge_prompt = KNOWLEDGE_TEMPLATE.format(knowledge='\n\n'.join(snippets))

        #logger.info(f"ææ–™ä¸­å‡ºç°å…³é”®ä¿¡æ¯: {keyword_prompt}")


        base_info_prompt = BASE_INFO_TEMPLATE.format(
            current_time=get_current_time(),
            three_month=get_three_month_ago(),
            last_year=get_last_year(),
            current_year=get_current_year()
        )


        if knowledge_prompt:
            if messages and messages[0][ROLE] == SYSTEM:
                if isinstance(messages[0][CONTENT], str):
                    messages[0][CONTENT] += '\n\n' + knowledge_prompt + '\n\n'
                else:
                    assert isinstance(messages[0][CONTENT], list)
                    messages[0][CONTENT] += [ContentItem(text='\n\n' + knowledge_prompt + '\n\n' )]
            else:
                # é‡æ–°ç»„åˆ System Promptï¼Œç¡®ä¿é€»è¾‘è¿è´¯
                full_system_content = (
                    f"{DEFAULT_SYSTEM_MESSAGE}\n\n"
                    f"{base_info_prompt}\n\n"  # åŸºç¡€ä¿¡æ¯å‰ç½®ï¼Œå»ºç«‹æ—¶é—´ä¸Šä¸‹æ–‡
                    f"{knowledge_prompt}\n\n"
                    f"{knowledge_graph_prompt}\n\n"
                    f"{excel_data_prompt}"
                )
                messages = [Message(role=SYSTEM, content=full_system_content), messages[-1]]
        self.source = references

        #logger.info(f'æœ€åæç¤ºè¯:{messages[0][CONTENT]}')
        return messages
    



        
    
    def _run_openai_format(
        self,
        messages: List[Message],
        lang: Literal['en', 'zh'] = 'zh',
        knowledge: str = '',
        **kwargs
    ) -> Iterator[str]:
        """Q&A with RAG and tool use abilities in OpenAI format.

        Args:
            knowledge: If an external knowledge string is provided,
              it will be used directly without retrieving information from files in messages.

        """
        # ä½¿ç”¨ä¸ _run ç›¸åŒçš„é€»è¾‘
        new_messages = self._prepend_knowledge_prompt(messages=messages, lang=lang, knowledge=knowledge, **kwargs)
        #logger.info(f'new_messages:{new_messages}')

        chunk_id = f"chatcmpl-{uuid.uuid4().hex}"
        created = int(time.time())
        model = "xmtelecom"
        # å‘é€obså¸§ - æ£€æŸ¥æ˜¯å¦æœ‰å®è´¨æ€§çš„çŸ¥è¯†åº“å†…å®¹
        # no_response = True #ä¸Šçº¿å‰æ”¹True
        if bool(self.source):
            #no_response = False 
            obs_chunk  = {
                    "id": chunk_id,
                    "object": "chat.completion.observation",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": json.dumps(self.source,ensure_ascii=False)},
                        "finish_reason": None
                    }]
                }
            yield f"data: {json.dumps(obs_chunk, ensure_ascii=False)}\n\n"
        else:
            logger.info('Skipping obs chunk due to insufficient content')

        

        # è°ƒç”¨çˆ¶ç±»çš„ _run æ–¹æ³•ï¼Œä½†è½¬æ¢è¾“å‡ºæ ¼å¼ä¸º OpenAI æµå¼æ ¼å¼
        chunk_id = f"chatcmpl-{uuid.uuid4().hex}"
        model = "xmtelecom"

        # å‘é€å¼€å§‹å¸§
        start_chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {"role": "assistant"},
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(start_chunk, ensure_ascii=False)}\n\n"




        # ä¸»è¦å›ç­”ç”Ÿæˆ
        try:
            # ç”Ÿæˆä¸»è¦å›ç­”ï¼Œä¸ä¼ é€’prev_full_texté¿å…é‡å¤
            yield from self.call_llm_with_messages(chunk_id=chunk_id,
                                                   model=model,
                                                   messages=new_messages,
                                                   lang='zh')

        except Exception as e:
            logger.error(f"Error in main response generation: {e}")
            # å‘é€é”™è¯¯æ¶ˆæ¯ç»™ç”¨æˆ·
            error_chunk = {
                "id": chunk_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": "\næŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
        

        # å‘é€ç»“æŸå¸§
        final_chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
        #yield "data: [DONE]\n\n"


    def call_llm_with_messages(self, chunk_id, model, messages: List[Message], lang, **kwargs):
        """
        è°ƒç”¨LLMç”Ÿæˆæµå¼å“åº”

        Args:
            prev_full_text: ä¹‹å‰çš„æ–‡æœ¬å†…å®¹ï¼ˆé¿å…é‡å¤è¾“å‡ºæ—¶ä½¿ç”¨ï¼‰
            is_supplement: æ˜¯å¦ä¸ºè¡¥å……è¯´æ˜
        """
        for message_batch in super()._run(messages=messages, lang=lang, **kwargs):
            if message_batch and message_batch[-1]:
                content = message_batch[-1].get(CONTENT, '')
                if content:
                    if isinstance(content, str):
                        text_content = content
                    else:
                        # å¤„ç† ContentItem åˆ—è¡¨
                        text_content = ""
                        for item in content if isinstance(content, list) else []:
                            if hasattr(item, 'text'):
                                text_content += item.text

                    
                    self.full_text = text_content
                    self.sources = self._extract_content_ref(text_content)
                    delta = {"content": text_content}
                    chunk = {
                        "id": chunk_id,
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": delta,
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
        # å¸¦ç´¢å¼•ï¼š
        
        if self.sources:
            references = [k['reference'] for k in self.knowledge_data if k['url'] in self.sources and k['reference'] is not None]
            reference = []
            for k in references:
                item = k.split('\n')
                for i in item:
                    if i not in reference:
                        reference.append(i)
            self.supp_text = "\n\n".join(reference)
            if len(reference):
                delta = {"content": f'{self.full_text}\n\n**å‚è€ƒå‡ºå¤„**\n\n{self.supp_text}'}
            else:
                delta = {"content": f'{self.full_text}\n\n'}
            #delta = { "content": f'{self.full_text}',"source": reference}
            
            chunk = {
                "id": chunk_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": delta,
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    def _extract_content_ref(self, full_text: str) -> List[str]:
        """æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰çš„å­—ç¬¦ä¸²
            ä¾‹å¦‚ [æ¥æº: [3](3)] ä½ åº”è¯¥å¾—åˆ° [3]

            [æ¥æº: [2](2), [7](7),[34](34),[46](46),[graph_chunk](graph_chunk), +more)]ã€‚
            å¾—åˆ° [2,7,34,46,graph_chunk]

            [æ¥æº: [doc_12579] å¾—åˆ° doc_12579
        """
        import re

        result = []
        seen = set()

        # æ¨¡å¼1: åŒ¹é… [æ¥æº: [å†…å®¹](é“¾æ¥)] æ ¼å¼
        pattern1 = r'\[æ¥æº:\s*\[([^\]]+)\]\([^)]+\)\]'
        matches1 = re.findall(pattern1, full_text)

        # æ¨¡å¼2:
        pattern2 = r'(?:doc_\d{5}|\d{5})'
        matches2 = re.findall(pattern2, full_text)

        # åˆå¹¶æ‰€æœ‰åŒ¹é…ç»“æœ
        all_matches = matches1 + matches2

        # å»é‡å¹¶ä¿æŒé¡ºåº
        for match in all_matches:
            if match not in seen:
                seen.add(match)
                result.append(match)

        return result