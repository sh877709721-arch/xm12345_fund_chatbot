import json
from enum import Enum
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from pydantic import BaseModel
from typing import List, Optional, Any
from uuid import uuid4
import time
import logging
import json
from starlette.background import BackgroundTask
from fastapi.responses import StreamingResponse
from app.service.chat import save_observation_message, update_chat_message, update_chat_message_background, save_observation_message_background
from app.core.graph.query_graphrag import rag_chatbot_local_search_stream
from collections import deque
# é¿å…å¾ªç¯å¯¼å…¥ï¼šåœ¨å‡½æ•°å†…éƒ¨åŠ¨æ€å¯¼å…¥
from app.model.message_context import ContextType


def determine_context_type(chunk_data: dict) -> ContextType:
    """
    æ ¹æ® chunk æ•°æ®ç¡®å®šå¯¹åº”çš„ ContextType

    Args:
        chunk_data: æµå¼å“åº”ä¸­çš„ chunk æ•°æ®å­—å…¸

    Returns:
        å¯¹åº”çš„ ContextType æšä¸¾å€¼
    """
    if not isinstance(chunk_data, dict):
        chunk_data = {}

    object_type = chunk_data.get("object", "")

    # æ ¹æ®ä¸åŒçš„ object ç±»å‹æ˜ å°„åˆ°ç›¸åº”çš„ ContextType
    if object_type == "chat.completion.question":
        return ContextType.question
    elif object_type == "chat.completion.observation":
        return ContextType.observation
    elif object_type == "chat.completion.thought":
        return ContextType.thought
    elif object_type == "chat.completion.action":
        return ContextType.action
    elif object_type == "chat.completion.summary":
        return ContextType.summary
    else:
        # é»˜è®¤ä¸º observation ç±»å‹
        return ContextType.observation


def create_context_chunk(object_type: str, content: Any, model: str | None = None, **kwargs) -> dict:
    """
    åˆ›å»ºæ ‡å‡†åŒ–çš„ä¸Šä¸‹æ–‡ chunk

    Args:
        object_type: å¯¹è±¡ç±»å‹ (question, observation, thought, action, summary)
        content: å†…å®¹æ•°æ®
        model: æ¨¡å‹åç§°
        **kwargs: é¢å¤–çš„å‚æ•°

    Returns:
        æ ‡å‡†åŒ–çš„ chunk å­—å…¸
    """
    chunk_id = f"chatcmpl-{uuid4().hex}"

    base_chunk = {
        "id": chunk_id,
        "object": f"chat.completion.{object_type}",
        "created": int(time.time()),
        "model": model or f"{object_type}_model",
        **kwargs
    }

    # æ ¹æ®ç±»å‹å¤„ç†å†…å®¹æ ¼å¼

    base_chunk["content"] = content

    return base_chunk


def save_context_chunk_by_type(chat_id: str, 
                               chunk_data: dict | str, 
                               assistant_message_id: int=0,
                               context_type: ContextType | None = None):
    """
    æ ¹æ®ç±»å‹ä¿å­˜ä¸Šä¸‹æ–‡ chunk

    Args:
        chat_id: èŠå¤©ä¼šè¯ID
        chunk_data: chunk æ•°æ®ï¼ˆå¯ä»¥æ˜¯å­—å…¸æˆ–å­—ç¬¦ä¸²ï¼‰
        context_type: æŒ‡å®šçš„ä¸Šä¸‹æ–‡ç±»å‹ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨æ¨æ–­
    """
    # å¦‚æœæ²¡æœ‰æŒ‡å®š context_typeï¼Œå°è¯•æ¨æ–­
    if context_type is None:
        if isinstance(chunk_data, dict):
            context_type = determine_context_type(chunk_data)
        else:
            # é»˜è®¤ä¸º observation ç±»å‹
            context_type = ContextType.observation

    # ç¡®ä¿ chunk_data æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå‡½æ•°æœŸæœ›çš„æ ¼å¼ï¼‰
    if isinstance(chunk_data, dict):
        # ç¡®ä¿åŒ…å«æ­£ç¡®çš„ object ç±»å‹
        if "object" not in chunk_data:
            object_type_map = {
                ContextType.question: "chat.completion.question",
                ContextType.observation: "chat.completion.observation",
                ContextType.thought: "chat.completion.thought",
                ContextType.action: "chat.completion.action",
                ContextType.summary: "chat.completion.summary"
            }
            chunk_data["object"] = object_type_map.get(context_type, "chat.completion.observation")

        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
        chunk_str = f"data: {json.dumps(chunk_data, ensure_ascii=False)}"
    else:
        chunk_str = chunk_data

    save_observation_message_background(chat_id, assistant_message_id,chunk_str, context_type)


def create_question_chunk(question_data: list, model: str = "similary_query") -> dict:
    """
    åˆ›å»º chat.completion.question ç±»å‹çš„ chunk

    Args:
        question_data: é—®é¢˜æ•°æ®åˆ—è¡¨
        model: æ¨¡å‹åç§°

    Returns:
        æ ‡å‡†åŒ–çš„ question chunk
    """
    return create_context_chunk(
        object_type="question",
        content=question_data,
        model=model
    )


def create_observation_chunk(observation_content: str | dict, model: str = "observation_model") -> dict:
    """
    åˆ›å»º chat.completion.observation ç±»å‹çš„ chunk

    Args:
        observation_content: è§‚å¯Ÿå†…å®¹
        model: æ¨¡å‹åç§°

    Returns:
        æ ‡å‡†åŒ–çš„ observation chunk
    """
    return create_context_chunk(
        object_type="observation",
        content=observation_content,
        model=model
    )


def create_thought_chunk(thought_content: str, model: str = "thought_model") -> dict:
    """
    åˆ›å»º chat.completion.thought ç±»å‹çš„ chunk

    Args:
        thought_content: æ€è€ƒå†…å®¹
        model: æ¨¡å‹åç§°

    Returns:
        æ ‡å‡†åŒ–çš„ thought chunk
    """
    return create_context_chunk(
        object_type="thought",
        content=thought_content,
        model=model
    )


def create_action_chunk(action_content: str | dict, model: str = "action_model") -> dict:
    """
    åˆ›å»º chat.completion.action ç±»å‹çš„ chunk

    Args:
        action_content: åŠ¨ä½œå†…å®¹
        model: æ¨¡å‹åç§°

    Returns:
        æ ‡å‡†åŒ–çš„ action chunk
    """
    return create_context_chunk(
        object_type="action",
        content=action_content,
        model=model
    )


def create_summary_chunk(summary_content: str, model: str = "summary_model") -> dict:
    """
    åˆ›å»º chat.completion.summary ç±»å‹çš„ chunk

    Args:
        summary_content: æ€»ç»“å†…å®¹
        model: æ¨¡å‹åç§°

    Returns:
        æ ‡å‡†åŒ–çš„ summary chunk
    """
    return create_context_chunk(
        object_type="summary",
        content=summary_content,
        model=model
    )


class ClientAttachment(BaseModel):
    name: str
    contentType: str
    url: str

class ToolInvocationState(str, Enum):
    CALL = 'call'
    PARTIAL_CALL = 'partial-call'
    RESULT = 'result'

class ToolInvocation(BaseModel):
    state: ToolInvocationState
    toolCallId: str
    toolName: str
    args: Any
    result: Any


class ClientMessage(BaseModel):
    role: str
    content: str
    experimental_attachments: Optional[List[ClientAttachment]] = None
    toolInvocations: Optional[List[ToolInvocation]] = None

class ChatRequest(BaseModel):
    messages: List[ClientMessage]

def convert_to_openai_messages(messages: List[ClientMessage]) -> List[ChatCompletionMessageParam]:
    openai_messages = []

    for message in messages:
        parts = []
        tool_calls = []

        parts.append({
            'type': 'text',
            'text': message.content
        })

        if (message.experimental_attachments):
            for attachment in message.experimental_attachments:
                if (attachment.contentType.startswith('image')):
                    parts.append({
                        'type': 'image_url',
                        'image_url': {
                            'url': attachment.url
                        }
                    })

                elif (attachment.contentType.startswith('text')):
                    parts.append({
                        'type': 'text',
                        'text': attachment.url
                    })

        if(message.toolInvocations):
            for toolInvocation in message.toolInvocations:
                tool_calls.append({
                    "id": toolInvocation.toolCallId,
                    "type": "function",
                    "function": {
                        "name": toolInvocation.toolName,
                        "arguments": json.dumps(toolInvocation.args)
                    }
                })

        tool_calls_dict = {"tool_calls": tool_calls} if tool_calls else {"tool_calls": None}

        openai_messages.append({
            "role": message.role,
            "content": parts,
            **tool_calls_dict,
        })

        if(message.toolInvocations):
            for toolInvocation in message.toolInvocations:
                tool_message = {
                    "role": "tool",
                    "tool_call_id": toolInvocation.toolCallId,
                    "content": json.dumps(toolInvocation.result),
                }

                openai_messages.append(tool_message)

    return openai_messages



def _get_similarity_questions(query: str, used_id: int = -1, top_n: int = 3) -> list:
    """
    å†…éƒ¨å‡½æ•°ï¼šè·å–ç›¸ä¼¼é—®é¢˜åˆ—è¡¨

    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        used_id: æ’é™¤çš„ID
        top_n: è¿”å›ç»“æœæ•°é‡

    Returns:
        ç›¸ä¼¼é—®é¢˜åˆ—è¡¨
    """
    try:
        # åŠ¨æ€å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
        from app.core.vector import get_adaptive_similarity_threshold_with_rerank_fallback
        return get_adaptive_similarity_threshold_with_rerank_fallback(query, used_id, top_n)
    except ImportError as e:
        logging.error(f"æ— æ³•å¯¼å…¥ç›¸ä¼¼åº¦æœç´¢å‡½æ•°: {e}")
        return []


def qa_stream_response(chat_id, qa_res, db, user_message_id, assistant_message_id):
    # ç«‹å³ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“è·å–ID
    final_text = qa_res[0]["answer"]
    saved_message = None

    try:
        saved_message = update_chat_message(chat_id, assistant_message_id, final_text, db)
        db_id = saved_message.id if saved_message else None
        logging.info(f"Pre-saved assistant message to DB with ID: {db_id}")
    except Exception as e:
        logging.exception("Failed to pre-save assistant message: %s", e)

    def qa_stream_agent():
        if user_message_id:
            user_id_chunk = {
                "id": f"user-msg-id-{uuid4().hex}",
                "object": "chat.completion.message_id",
                "created": int(time.time()),
                "model": 'qa_model',
                "message_id": {
                    "user_message_id": user_message_id,
                    "assistant_message_id": assistant_message_id,
                }
            }
            yield f"data: {json.dumps(user_id_chunk, ensure_ascii=False)}\n\n"

        chunk_id = f"chatcmpl-{uuid4().hex}"
        chunk = {
                    "id": chunk_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": 'qa_model',
                    "choices": [{
                        "index": 0,
                        "delta": {"content": final_text},
                        "finish_reason": None
                    }]
                }
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
        chunk = {
                    "id": chunk_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": 'qa_model',
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

        yield f"data: [DONE]\n\n"

    return StreamingResponse(qa_stream_agent(), media_type="text/plain")


def qa_stream_response_optimized(chat_id, query, qa_res, user_message_id, assistant_message_id):
    """
    ğŸ”§ **ä¼˜åŒ–ç‰ˆæœ¬QAæµå¼å“åº”**

    ä¼˜åŒ–ç‰¹ç‚¹ï¼š
    1. ä¸æŒæœ‰æ•°æ®åº“è¿æ¥è¿›è¡Œæµå¼ä¼ è¾“
    2. ä½¿ç”¨åå°ä»»åŠ¡æ›´æ–°æœ€ç»ˆæ¶ˆæ¯å†…å®¹
    3. å‡å°‘è¿æ¥æ± å ç”¨æ—¶é—´
    """
    final_text = qa_res[0]["answer"]
    used_id = qa_res[0]["id"]
    reference =  f'\n\n**å‚è€ƒå‡ºå¤„**: \n\n {qa_res[0]["reference"]}' if qa_res[0]["reference"] else '' # "å‚è€ƒæ¥æº:"
    final_content = f"{final_text}\n\n{reference}"

    # æ ¼å¼åŒ–æ–‡æœ¬ä¸ºMarkdownå‹å¥½çš„æ¢è¡Œæ ¼å¼
    from app.core.text_formatter import format_text_for_markdown
    final_content = format_text_for_markdown(final_content)
    observation_chunks = []  # æ”¶é›†è§‚å¯Ÿæ¶ˆæ¯ç”¨äºåå°å¤„ç†
    similary_query = _get_similarity_questions(query, used_id, top_n=3)
    chunk_id = f"chatcmpl-{uuid4().hex}"  # ä¸ºæµå¼å“åº”ç”Ÿæˆç»Ÿä¸€çš„ chunk ID
    # ä½¿ç”¨æ–°çš„ ContextType å°è£…åˆ›å»º question chunk
    question_chunk = create_question_chunk(
        question_data=similary_query,
        model="similary_query"
    )

    observation_chunks.append(question_chunk)
    

    def qa_stream_agent():
        # å‘é€æ¶ˆæ¯ID
        if user_message_id:
            user_id_chunk = {
                "id": f"user-msg-id-{uuid4().hex}",
                "object": "chat.completion.message_id",
                "created": int(time.time()),
                "model": 'qa_model',
                "message_id": {
                    "user_message_id": user_message_id,
                    "assistant_message_id": assistant_message_id,
                }
            }
            yield f"data: {json.dumps(user_id_chunk, ensure_ascii=False)}\n\n"

        # æµå¼ä¼ è¾“å†…å®¹
        
        chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": 'qa_model',
            "choices": [{
                "index": 0,
                "delta": {"content": final_content},
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

        # ç»“æŸæ ‡è®°
        chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": 'qa_model',
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

        yield f"data: [DONE]\n\n"

    # ğŸ”§ **å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨åå°ä»»åŠ¡æ›´æ–°æ•°æ®åº“ï¼Œä¸å ç”¨æµå¼è¿æ¥**
    def background_update():
        for chunk in observation_chunks:
            # ä½¿ç”¨æ–°çš„ ContextType å°è£…ä¿å­˜ chunk
            
            save_context_chunk_by_type(chat_id, chunk,assistant_message_id,None)  # è‡ªåŠ¨æ¨æ–­ ContextType

        update_chat_message_background(chat_id, assistant_message_id, content=final_content)


    return StreamingResponse(
        qa_stream_agent(),
        media_type="text/plain",
        background=BackgroundTask(background_update)
    )


def agent_stream_response(chat_id, bot, final_content, agent_messages, db, user_message_id,assistant_message_id):
    #logging.info(f'agent_message:{agent_messages}')
    def stream_agent():
        # é¦–å…ˆå‘é€ç”¨æˆ·æ¶ˆæ¯IDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if user_message_id:
            user_id_chunk = {
                "id": f"user-msg-id-{uuid4().hex}",
                "object": "chat.completion.message_id",
                "created": int(time.time()),
                "model": 'agent_model',
                "message_id": {
                    "user_message_id": user_message_id,
                    "assistant_message_id": assistant_message_id,
                }
            }
            yield f"data: {json.dumps(user_id_chunk, ensure_ascii=False)}\n\n"

        # è¿™é‡Œåº”è¯¥ä½¿ç”¨å®é™…çš„æ¶ˆæ¯è€Œä¸æ˜¯ç¡¬ç¼–ç 
        recent_chunks = deque(maxlen=3)

        # bot._run_openai_format may be a sync generator; iterate normally
        for chunk in bot._run_openai_format(agent_messages):
            # accumulate for later parsing
            recent_chunks.append(chunk)
            # yield chunk as-is to the client
            save_observation_message(chat_id, chunk, db)

            yield chunk

        # After streaming finished, try to extract the assistant's final text
        if len(recent_chunks) >= 1:
            # try to parse the last few chunks for a content field
            # prefer checking the last element that contains actual content
            parsed = ""
            try:
                # iterate from newest to oldest to find content
                for c in reversed(recent_chunks):
                    if isinstance(c, str) and c.startswith("data: "):
                        json_str = c[6:]
                        if json_str.strip() == "[DONE]":
                            continue
                        chunk_data = json.loads(json_str)
                        if "choices" in chunk_data and len(chunk_data["choices"]) > 0:
                            delta = chunk_data["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                parsed = content
                                break
                if parsed:
                    #logging.info(f'Parsed final assistant content: {parsed}')
                    final_content["text"] = parsed
            except Exception as e:
                logging.warning(f"Error parsing chunks for final content: {e}")

        # ç«‹å³ä¿å­˜æœ€ç»ˆæ¶ˆæ¯å¹¶è·å–æ•°æ®åº“ID
        text = final_content.get("text")
        if text:
            try:
                saved_message = update_chat_message(chat_id,assistant_message_id, text, db)
                db_id = saved_message.id if saved_message else None
                final_content["db_id"] = db_id
                logging.info(f"Saved assistant final message to DB with ID: {db_id}")
            except Exception as e:
                logging.exception("Failed to save assistant message: %s", e)
                final_content["db_id"] = None

        yield f"data: [DONE]\n\n"
        logging.info(f'Final content to save: {final_content.get("text")}')

    # å¯¹äºagentæ¨¡å¼ï¼Œæ¶ˆæ¯ä¿å­˜å·²ç»åœ¨æµå†…å®Œæˆï¼Œåå°ä»»åŠ¡åªåšéªŒè¯
    def _verify_saved():
        pass

    return StreamingResponse(stream_agent(), media_type="text/plain", background=BackgroundTask(_verify_saved))


def agent_stream_response_optimized(chat_id, query, bot, agent_messages, user_message_id, assistant_message_id):
    """
    ğŸ”§ **ä¼˜åŒ–ç‰ˆæœ¬Agentæµå¼å“åº”**

    ä¼˜åŒ–ç‰¹ç‚¹ï¼š
    1. ä¸æŒæœ‰æ•°æ®åº“è¿æ¥è¿›è¡Œæµå¼ä¼ è¾“
    2. è§‚å¯Ÿæ¶ˆæ¯ä½¿ç”¨åå°ä»»åŠ¡å¼‚æ­¥ä¿å­˜
    3. æœ€ç»ˆæ¶ˆæ¯ä½¿ç”¨åå°ä»»åŠ¡æ›´æ–°
    4. å¤§å¹…å‡å°‘è¿æ¥æ± å ç”¨æ—¶é—´
    """
    # ğŸ”§ **ä¿®å¤ï¼šå°†æ•°æ®æ”¶é›†å™¨ç§»åˆ°å¤–éƒ¨ä½œç”¨åŸŸï¼Œç¡®ä¿åå°ä»»åŠ¡å¯ä»¥è®¿é—®**
    final_content = {"text": ""}
    observation_chunks = []  # æ”¶é›†è§‚å¯Ÿæ¶ˆæ¯ç”¨äºåå°å¤„ç†

    # å¯¹äº Agent æµå¼å“åº”ï¼Œæ²¡æœ‰ç›´æ¥çš„ used_idï¼Œä½¿ç”¨é»˜è®¤å€¼ -1
    similary_query = _get_similarity_questions(query, used_id=-1, top_n=3)
    chunk_id = f"chatcmpl-{uuid4().hex}"  # ä¸ºæµå¼å“åº”ç”Ÿæˆç»Ÿä¸€çš„ chunk ID
    # ä½¿ç”¨æ–°çš„ ContextType å°è£…åˆ›å»º question chunk
    question_chunk = create_question_chunk(
        question_data=similary_query,
        model="similary_query"
    )

    observation_chunks.append(question_chunk)

    def stream_agent():
        # å‘é€æ¶ˆæ¯ID
        if user_message_id:
            user_id_chunk = {
                "id": f"user-msg-id-{uuid4().hex}",
                "object": "chat.completion.message_id",
                "created": int(time.time()),
                "model": 'agent_model',
                "message_id": {
                    "user_message_id": user_message_id,
                    "assistant_message_id": assistant_message_id,
                }
            }
            yield f"data: {json.dumps(user_id_chunk, ensure_ascii=False)}\n\n"

        # æ”¶é›†æµå¼æ•°æ®ç”¨äºæœ€ç»ˆå†…å®¹è§£æ
        recent_chunks = deque(maxlen=3)

        # ğŸ”§ **å…³é”®ä¼˜åŒ–ï¼šæµå¼ä¼ è¾“æœŸé—´ä¸è¿›è¡Œæ•°æ®åº“æ“ä½œ**
        for chunk in bot._run_openai_format(agent_messages):
            recent_chunks.append(chunk)

            # æ”¶é›†è§‚å¯Ÿæ¶ˆæ¯ç±»å‹çš„æ•°æ®ï¼Œç•™å¾…åå°å¤„ç†
            if chunk.startswith("data: "):
                try:
                    json_str = chunk[6:].strip()
                    if json_str != "[DONE]":
                        chunk_data = json.loads(json_str)
                        if chunk_data.get("object") == "chat.completion.observation":
                            observation_chunks.append(chunk)
                except json.JSONDecodeError:
                    pass  # å¿½ç•¥è§£æé”™è¯¯ï¼Œç»§ç»­æµå¼ä¼ è¾“

            # ç«‹å³å‘å®¢æˆ·ç«¯å‘é€æ•°æ®
            yield chunk

        # è§£ææœ€ç»ˆæ¶ˆæ¯å†…å®¹
        parsed_content = ""
        if len(recent_chunks) >= 1:
            try:
                for c in reversed(recent_chunks):
                    if isinstance(c, str) and c.startswith("data: "):
                        json_str = c[6:]
                        if json_str.strip() == "[DONE]":
                            continue
                        chunk_data = json.loads(json_str)
                        if "choices" in chunk_data and len(chunk_data["choices"]) > 0:
                            delta = chunk_data["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                parsed_content = content
                                break
                if parsed_content:
                    #logging.info(f'Parsed final assistant content: {parsed_content}')
                    final_content["text"] = parsed_content
            except Exception as e:
                logging.warning(f"Error parsing chunks for final content: {e}")
        
        yield f"data: [DONE]\n\n"
        # logging.info(f'Final content extracted: {final_content.get("text")}')

    # ğŸ”§ **å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨åå°ä»»åŠ¡å¤„ç†æ‰€æœ‰æ•°æ®åº“æ“ä½œ**
    def background_update():
        # 1. å¼‚æ­¥ä¿å­˜è§‚å¯Ÿæ¶ˆæ¯ - æ ¹æ®ç±»å‹åˆ†ç±»å¤„ç†
        for chunk in observation_chunks:
            context_type = determine_context_type(chunk)
            save_context_chunk_by_type(chat_id,chunk,assistant_message_id,context_type)
            #save_observation_message_background(chat_id, chunk, context_type)

        # 2. æ›´æ–°æœ€ç»ˆæ¶ˆæ¯å†…å®¹
        final_text = final_content.get("text")
        if final_text:
            logging.info(f'Background update: saving final message content (length: {len(final_text)})')
            update_chat_message_background(chat_id, assistant_message_id, final_text)
        else:
            logging.warning('Background update: no final text to save')

    return StreamingResponse(
        stream_agent(),
        media_type="text/plain",
        background=BackgroundTask(background_update)
    )




def graphrag_stream_response_optimized(chat_id, query, user_message_id, assistant_message_id):
    """
    ğŸ”§ **ä¼˜åŒ–ç‰ˆæœ¬GraphRAGæµå¼å“åº”**

    éµå¾ªä¸ agent_stream_response_optimized ç›¸åŒçš„è®¾è®¡æ¨¡å¼å’Œæ¶ˆæ¯æ ¼å¼

    ä¼˜åŒ–ç‰¹ç‚¹ï¼š
    1. ä¸æŒæœ‰æ•°æ®åº“è¿æ¥è¿›è¡Œæµå¼ä¼ è¾“
    2. æœ€ç»ˆæ¶ˆæ¯ä½¿ç”¨åå°ä»»åŠ¡æ›´æ–°
    3. ä¸agentå“åº”æ ¼å¼å®Œå…¨ä¸€è‡´
    4. å¤§å¹…å‡å°‘è¿æ¥æ± å ç”¨æ—¶é—´
    """

    # ğŸ”§ **ä¿®å¤ï¼šå°†æ•°æ®æ”¶é›†å™¨ç§»åˆ°å¤–éƒ¨ä½œç”¨åŸŸï¼Œç¡®ä¿åå°ä»»åŠ¡å¯ä»¥è®¿é—®**
    final_content = {"text": ""}

    async def stream_graphrag():
        # å‘é€æ¶ˆæ¯ID - ä¸ agent_stream_response_optimized æ ¼å¼ä¸€è‡´
        if user_message_id:
            user_id_chunk = {
                "id": f"user-msg-id-{uuid4().hex}",
                "object": "chat.completion.message_id",
                "created": int(time.time()),
                "model": 'graphrag_model',
                "message_id": {
                    "user_message_id": user_message_id,
                    "assistant_message_id": assistant_message_id,
                }
            }
            yield f"data: {json.dumps(user_id_chunk, ensure_ascii=False)}\n\n"

        start_chunk = {
            "id": f"chatcmpl-{uuid4().hex}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "graphrag-boost",
            "choices": [
                {
                    "index": 0,
                    "delta": {
                        "content": "æ­£åœ¨ä¸ºæ‚¨æ£€ç´¢æ•°æ®...."
                    },
                    "finish_reason": None
                }
            ]
        }

        chunk_str = f"data: {json.dumps(start_chunk, ensure_ascii=False)}\n\n"
        yield chunk_str

        # æ”¶é›†æµå¼æ•°æ®ç”¨äºæœ€ç»ˆå†…å®¹è§£æ
        recent_chunks = deque(maxlen=3)
        accumulated_content = ""

        # ğŸ”§ **å…³é”®ä¼˜åŒ–ï¼šæµå¼ä¼ è¾“æœŸé—´ä¸è¿›è¡Œæ•°æ®åº“æ“ä½œ**
        try:
            # æ‰§è¡Œ GraphRAG æœ¬åœ°æœç´¢æµå¼æŸ¥è¯¢ rag_chatbot_local_search_stream
            async for chunk in rag_chatbot_local_search_stream(query):
                if chunk:  # ç¡®ä¿ä¸ä¸ºç©º
                    accumulated_content += chunk

                    # æ„é€ æ ‡å‡†çš„ OpenAI æ ¼å¼å“åº”å—
                    response_chunk = {
                        "id": f"chatcmpl-{uuid4().hex}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": "graphrag-boost",
                        "choices": [
                            {
                                "index": 0,
                                "delta": {
                                    "content": accumulated_content
                                },
                                "finish_reason": None
                            }
                        ]
                    }

                    chunk_str = f"data: {json.dumps(response_chunk, ensure_ascii=False)}\n\n"
                    recent_chunks.append(chunk_str)

                    # ç«‹å³å‘å®¢æˆ·ç«¯å‘é€æ•°æ®
                    yield chunk_str

        except Exception as e:
            logging.error(f"GraphRAG æµå¼æŸ¥è¯¢é”™è¯¯: {str(e)}")
            # å‘é€é”™è¯¯å—
            error_chunk = {
                "id": f"chatcmpl-{uuid4().hex}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": "graphrag-boost",
                "choices": [
                    {
                        "index": 0,
                        "delta": {
                            "content": f"GraphRAG å¤„ç†å¤±è´¥: {str(e)}"
                        },
                        "finish_reason": "error"
                    }
                ]
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

        # è§£ææœ€ç»ˆæ¶ˆæ¯å†…å®¹
        parsed_content = accumulated_content  # GraphRAG ç›´æ¥ç´¯ç§¯å†…å®¹
        if parsed_content:
            final_content["text"] = parsed_content

        # å‘é€å®Œæˆæ ‡è®°
        done_chunk = {
            "id": f"chatcmpl-{uuid4().hex}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "graphrag-boost",
            "choices": [
                {
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }
            ]
        }
        yield f"data: {json.dumps(done_chunk, ensure_ascii=False)}\n\n"
        yield f"data: [DONE]\n\n"

        logging.info(f"GraphRAG æµå¼å“åº”å®Œæˆï¼Œchat_id: {chat_id}, query: {query[:50]}..., å“åº”é•¿åº¦: {len(parsed_content)} å­—ç¬¦")

    # ğŸ”§ **å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨åå°ä»»åŠ¡å¤„ç†æ‰€æœ‰æ•°æ®åº“æ“ä½œ**
    def background_update():
        # æ›´æ–°æœ€ç»ˆæ¶ˆæ¯å†…å®¹
        final_text = final_content.get("text")
        if final_text:
            logging.info(f'GraphRAG Background update: saving final message content (length: {len(final_text)})')
            update_chat_message_background(chat_id, assistant_message_id, final_text)
        else:
            logging.warning('GraphRAG Background update: no final text to save')

    return StreamingResponse(
        stream_graphrag(),
        media_type="text/plain",
        background=BackgroundTask(background_update)
    )


def test_context_type_functions():
    """
    æµ‹è¯•æ–°çš„ ContextType å°è£…åŠŸèƒ½
    """
    print("ğŸ§ª æµ‹è¯• ContextType å°è£…åŠŸèƒ½")

    # æµ‹è¯•ä¸åŒç±»å‹çš„ chunk åˆ›å»º
    test_cases = [
        ("question", [{"id": 1, "question": "æµ‹è¯•é—®é¢˜", "answer": "æµ‹è¯•ç­”æ¡ˆ"}]),
        ("observation", "è¿™æ˜¯è§‚å¯Ÿå†…å®¹"),
        ("thought", "è¿™æ˜¯æ€è€ƒè¿‡ç¨‹"),
        ("action", {"action": "search", "params": {"query": "test"}}),
        ("summary", "è¿™æ˜¯å¯¹è¯æ€»ç»“")
    ]

    for obj_type, content in test_cases:
        chunk = create_context_chunk(obj_type, content, model=f"test_{obj_type}")
        print(f"âœ… {obj_type} chunk åˆ›å»ºæˆåŠŸ: {chunk['object']}")

        # æµ‹è¯•ç±»å‹æ¨æ–­
        inferred_type = determine_context_type(chunk)
        expected_type = getattr(ContextType, obj_type)
        assert inferred_type == expected_type, f"ç±»å‹æ¨æ–­é”™è¯¯: {inferred_type} != {expected_type}"
        print(f"  âœ… ç±»å‹æ¨æ–­æ­£ç¡®: {inferred_type}")

    print("\nğŸ‰ æ‰€æœ‰ ContextType å°è£…åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")

    # è¿”å›ä¸€ä¸ªç¤ºä¾‹ question chunk
    sample_question_data = [
        {"id": 1, "question": "AIæ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "äººå·¥æ™ºèƒ½æ˜¯æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚"},
        {"id": 2, "question": "æœºå™¨å­¦ä¹ åŸç†ï¼Ÿ", "answer": "æœºå™¨å­¦ä¹ é€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹æ¥åšå‡ºé¢„æµ‹ã€‚"}
    ]

    return create_question_chunk(sample_question_data, "qa_model")